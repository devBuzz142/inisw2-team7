import sys, time, os, tqdm, torch, argparse, glob, subprocess, warnings, cv2, pickle, numpy, pdb, math, python_speech_features, time

from scipy import signal
from shutil import rmtree
from scipy.io import wavfile
from scipy.interpolate import interp1d

from scenedetect.video_manager import VideoManager
from scenedetect.scene_manager import SceneManager
from scenedetect.frame_timecode import FrameTimecode
from scenedetect.stats_manager import StatsManager
from scenedetect.detectors import ContentDetector

from model.faceDetector.s3fd import S3FD
from talkNet import talkNet


warnings.filterwarnings("ignore")




parser = argparse.ArgumentParser(description = "Columbia ASD Evaluation")

parser.add_argument('--videoName',             type=str, default="quiz")
parser.add_argument('--videoFolder',           type=str, default="demo")
parser.add_argument('--ASD_Model',             type=str, default="sevenTeam_TalkNet.model")

parser.add_argument('--nDataLoaderThread',     type=int,   default=10 )
parser.add_argument('--facedetScale',          type=float, default=0.25, help='face detection scaleê³„ìˆ˜ ì„¤ì •' )
parser.add_argument('--minTrack',              type=int,   default=10 )
parser.add_argument('--numFailedDet',          type=int,   default=10 )
parser.add_argument('--minFaceSize',           type=int,   default=1 )
parser.add_argument('--cropScale',             type=float, default=0.40 )

parser.add_argument('--start',                 type=int, default=0 )
parser.add_argument('--duration',              type=int, default=0 )

args = parser.parse_args()


args.videoPath = glob.glob(os.path.join(args.videoFolder, args.videoName + '.*'))[0]
args.savePath = os.path.join(args.videoFolder, args.videoName)

def scene_detect(args):
	# CPU: ì¥ë©´ ê°ì§€, ì¶œë ¥ì€ ê° ì´¬ì˜ ì‹œê°„ ê¸°ê°„ì˜ ëª©ë¡
	videoManager = VideoManager([args.videoFilePath])		# ë¹„ë””ì˜¤ load. ê´€ë¦¬í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë¹„ë””ì˜¤ì˜ ê²½ë¡œ
	statsManager = StatsManager()							# StatsManager ìƒì„±. ë¹„ë””ì˜¤ì˜ ê° ì¥ë©´ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¶”ì í•˜ëŠ” ë° ì‚¬ìš©
	sceneManager = SceneManager(statsManager)				# StatsManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ë©´ ê´€ë¦¬ìë¥¼ ìƒì„±. ì¥ë©´ ê°ì§€ì™€ ì •ë³´ ìˆ˜ì§‘ì„ ê´€ë¦¬
	sceneManager.add_detector(ContentDetector())			# ì»¨í…ì¸  ê°ì§€ê¸°ë¥¼ ì¶”ê°€í•˜ì—¬, ì¥ë©´ì´ ë°”ë€ŒëŠ” ì‹œì ì„ ê°ì§€í•˜ëŠ” ê¸°ëŠ¥ì„ í™œì„±í™”
	baseTimecode = videoManager.get_base_timecode()			# ë¹„ë””ì˜¤ì˜ ê¸°ë³¸ íƒ€ì„ì½”ë“œë¥¼ get. ë¹„ë””ì˜¤ ì‹œì‘ ì‹œì ì˜ íƒ€ì„ì½”ë“œ
	videoManager.set_downscale_factor()						# ë¹„ë””ì˜¤ì˜ ë‹¤ìš´ìŠ¤ì¼€ì¼ íŒ©í„°ë¥¼ ì„¤ì •. ë¹„ë””ì˜¤ ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©
	videoManager.start()									# ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ì‹œì‘
	sceneManager.detect_scenes(frame_source = videoManager)	# ë¹„ë””ì˜¤ì—ì„œ ì¥ë©´ì„ ê°ì§€
	sceneList = sceneManager.get_scene_list(baseTimecode)	# ì¥ë©´ ëª©ë¡ get. ê° ì¥ë©´ì€ ì‹œì‘ê³¼ ë íƒ€ì„ì½”ë“œë¡œ í‘œí˜„
	savePath = os.path.join(args.pyworkPath, 'scene.pckl')	# ì¥ë©´ ëª©ë¡ì„ ì €ì¥í•  íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì„¤ì •

	# ë§Œì•½ ê°ì§€ëœ ì¥ë©´ì´ ì—†ë‹¤ë©´, ë¹„ë””ì˜¤ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì¥ë©´ìœ¼ë¡œ ì·¨ê¸‰
	if sceneList == []:
		sceneList = [(videoManager.get_base_timecode(),videoManager.get_current_timecode())]

	# ì¥ë©´ ëª©ë¡ì„ íŒŒì¼ì— ì €ì¥.
	with open(savePath, 'wb') as fil:
		pickle.dump(sceneList, fil)
		sys.stderr.write('%s - scenes detected %d\n'%(args.videoFilePath, len(sceneList)))

	# ì¥ë©´ ëª©ë¡ ë°˜í™˜
	return sceneList

def inference_video(args):
	# GPU: Face detection, output is the list contains the face location and score in this frame
	DET = S3FD(device='cuda') 											# GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ ê°ì§€ ëª¨ë¸ì„ ì´ìš©
	flist = glob.glob(os.path.join(args.pyframesPath, '*.jpg'))			
	flist.sort()
	dets = []															# ê° í”„ë ˆì„ì˜ ì–¼êµ´ ê°ì§€ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
	for fidx, fname in enumerate(flist):
		image = cv2.imread(fname)										
		imageNumpy = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)				# ì´ë¯¸ì§€ë¥¼ BGRì—ì„œ RGBë¡œ ë³€í™˜.
		bboxes = DET.detect_faces(imageNumpy, conf_th=0.9, scales=[args.facedetScale])		# ê°ì§€ëœ ì–¼êµ´ì˜ bounding boxë¥¼ GET
		dets.append([])
		for bbox in bboxes:
		  dets[-1].append({'frame':fidx, 'bbox':(bbox[:-1]).tolist(), 'conf':bbox[-1]}) # í˜„ì¬ í”„ë ˆì„, ê²½ê³„ ìƒì, ì‹ ë¢°ë„ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
		sys.stderr.write('%s-%05d; %d dets\r' % (args.videoFilePath, fidx, len(dets[-1])))
	savePath = os.path.join(args.pyworkPath,'faces.pckl')				# í•„ìš” ì—†ìŒ
	with open(savePath, 'wb') as fil:									# í•„ìš” ì—†ìŒ
		pickle.dump(dets, fil)											# í•„ìš” ì—†ìŒ
	return dets

def bb_intersection_over_union(boxA, boxB):
	# CPU: # ë‘ ì´ë¯¸ì§€ì˜ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ ê³„ì‚°í•˜ëŠ” IOU í•¨ìˆ˜
	xA = max(boxA[0], boxB[0])
	yA = max(boxA[1], boxB[1])
	xB = min(boxA[2], boxB[2])
	yB = min(boxA[3], boxB[3])
	interArea = max(0, xB - xA) * max(0, yB - yA)
	boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
	boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
	iou = interArea / float(boxAArea + boxBArea - interArea)
	return iou

def track_shot(args, sceneFaces):
	# ì–¼êµ´ ì¶”ì 
	iouThres  = 0.5     # ì—°ì†ëœ ì–¼êµ´ ê°ì§€ ì‚¬ì´ì˜ ìµœì†Œ IOU ê°’
	tracks    = []		# ì¶”ì ëœ ì–¼êµ´ íŠ¸ë™ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
	while True:
		track     = []	# í˜„ì¬ ì¶”ì  ì¤‘ì¸ ì–¼êµ´ íŠ¸ë™ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
		for frameFaces in sceneFaces:		# ê° í”„ë ˆì„ì—ì„œì˜ ì–¼êµ´ ê°ì§€ ê²°ê³¼ì— ëŒ€í•´ ë°˜ë³µ
			for face in frameFaces:			# í”„ë ˆì„ ë‚´ì˜ ê° ì–¼êµ´ì— ëŒ€í•´ ë°˜ë³µ
				# í˜„ì¬ ì¶”ì  ì¤‘ì¸ íŠ¸ë™ì´ ë¹„ì–´ ìˆëŠ” ê²½ìš° í˜„ì¬ ì–¼êµ´ì„ íŠ¸ë™ì— ì¶”ê°€
				if track == []:				
					track.append(face)		
					frameFaces.remove(face)	# ì¶”ê°€í•œ ì–¼êµ´ì„ í˜„ì¬ í”„ë ˆì„ì—ì„œ ì œê±°

				# ì´ì „ ì–¼êµ´ê³¼ í˜„ì¬ ì–¼êµ´ ì‚¬ì´ì˜ í”„ë ˆì„ ê°„ê²©ì´ ì¼ì • ì´í•˜ì¸ ê²½ìš° IOUë¥¼ ê³„ì‚°
				elif face['frame'] - track[-1]['frame'] <= args.numFailedDet:			
					iou = bb_intersection_over_union(face['bbox'], track[-1]['bbox'])	
					# IOUê°€ ì§€ì •í•œ ì„ê³„ê°’ë³´ë‹¤ í° ê²½ìš° í˜„ì¬ ì–¼êµ´ì„ íŠ¸ë™ì— ì¶”ê°€
					if iou > iouThres:			
						track.append(face)		
						frameFaces.remove(face)	# ì¶”ê°€í•œ ì–¼êµ´ì„ í˜„ì¬ í”„ë ˆì„ì—ì„œ ì œê±°
						continue
				else:
					break		# ì´ì „ ì–¼êµ´ê³¼ í˜„ì¬ ì–¼êµ´ ì‚¬ì´ì˜ í”„ë ˆì„ ê°„ê²©ì´ ì§€ì •í•œ ê°’ë³´ë‹¤ í° ê²½ìš°, ë” ì´ìƒ íŠ¸ë™ì„ ì—°ì¥ X
		if track == []:			# ë” ì´ìƒ ì¶”ì í•  ì–¼êµ´ì´ ì—†ëŠ” ê²½ìš° ë°˜ë³µë¬¸ ì¢…ë£Œ
			break				
		elif len(track) > args.minTrack:		# íŠ¸ë™ì˜ ê¸¸ì´ê°€ ìµœì†Œ íŠ¸ë™ ê¸¸ì´ë³´ë‹¤ í° ê²½ìš°
			frameNum    = numpy.array([ f['frame'] for f in track ])		 	# íŠ¸ë™ ë‚´ì˜ ê° ì–¼êµ´ì˜ í”„ë ˆì„ ë²ˆí˜¸ë¥¼ ë°°ì—´ë¡œ ì €ì¥
			bboxes      = numpy.array([numpy.array(f['bbox']) for f in track])	# íŠ¸ë™ ë‚´ì˜ ê° ì–¼êµ´ì˜ ê²½ê³„ ìƒì ì¢Œí‘œë¥¼ ë°°ì—´ë¡œ ì €ì¥
			frameI      = numpy.arange(frameNum[0],frameNum[-1]+1)				# íŠ¸ë™ì˜ ì²« ë²ˆì§¸ í”„ë ˆì„ë¶€í„° ë§ˆì§€ë§‰ í”„ë ˆì„ê¹Œì§€ì˜ í”„ë ˆì„ ë²ˆí˜¸ ë°°ì—´ì„ ìƒì„±
			bboxesI    = []
			for ij in range(0,4):
				interpfn  = interp1d(frameNum, bboxes[:,ij])		# íŠ¸ë™ ë‚´ì˜ ê° ì¢Œí‘œ ì¶•ì— ëŒ€í•´ ì„ í˜• ë³´ê°„ í•¨ìˆ˜ë¥¼ ìƒì„±
				bboxesI.append(interpfn(frameI))					# ë³´ê°„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ì— ëŒ€í•œ ë³´ê°„ëœ ì¢Œí‘œë¥¼ ê³„ì‚°í•˜ê³  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
			bboxesI  = numpy.stack(bboxesI, axis=1)					# ë³´ê°„ëœ ì¢Œí‘œë¥¼ ë°°ì—´ë¡œ ë³€í™˜
			if max(numpy.mean(bboxesI[:,2]-bboxesI[:,0]), numpy.mean(bboxesI[:,3]-bboxesI[:,1])) > args.minFaceSize:	# ë³´ê°„ëœ ì¢Œí‘œ ì¤‘ ê°€ë¡œ ë˜ëŠ” ì„¸ë¡œ ê¸¸ì´ê°€ ìµœì†Œ ì–¼êµ´ í¬ê¸°ë³´ë‹¤ í° ê²½ìš°
				tracks.append({'frame':frameI,'bbox':bboxesI})		# íŠ¸ë™ì„ ì¶”ì  ëª©ë¡ì— ì¶”ê°€
	return tracks		# ì¶”ì ëœ ì–¼êµ´ íŠ¸ë™ì„ ë°˜í™˜


def crop_video(args, track, cropFile):
	# ì–¼êµ´ í´ë¦½ ìë¥´ê¸°
	flist = glob.glob(os.path.join(args.pyframesPath, '*.jpg')) # í”„ë ˆì„ë“¤ì„ ì½ì–´ì˜´
	flist.sort()
	vOut = cv2.VideoWriter(cropFile + 't.avi', cv2.VideoWriter_fourcc(*'XVID'), 25, (224,224))	# ë¹„ë””ì˜¤ë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•œ ê°ì²´ë¥¼ ìƒì„±
	dets = {'x':[], 'y':[], 's':[]}		# ì–¼êµ´ ê°ì§€ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”
	for det in track['bbox']: 
		dets['s'].append(max((det[3]-det[1]), (det[2]-det[0]))/2) 
		dets['y'].append((det[1]+det[3])/2) # yì˜ ì¤‘ì‹¬ì¢Œí‘œ ê³„ì‚°
		dets['x'].append((det[0]+det[2])/2) # xì˜ ì¤‘ì‹¬ì¢Œí‘œ ê³„ì‚°
	dets['s'] = signal.medfilt(dets['s'], kernel_size=13) 
	dets['x'] = signal.medfilt(dets['x'], kernel_size=13)
	dets['y'] = signal.medfilt(dets['y'], kernel_size=13)
	for fidx, frame in enumerate(track['frame']):
		cs  = args.cropScale	# í´ë¦½ì„ ìë¥¼ ë•Œ ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ ê°’ì„ GET
		bs  = dets['s'][fidx]   # ê°ì§€ëœ ì–¼êµ´ì˜ í¬ê¸°
		bsi = int(bs * (1 + 2 * cs))  # ì–¼êµ´ í´ë¦½ ì£¼ë³€ì— ì—¬ìœ  ê³µê°„ì„ ë‘ê¸° ìœ„í•´ ì–¼êµ´ í¬ê¸°ë¥¼ ì¡°ì •
		image = cv2.imread(flist[frame])	# í”„ë ˆì„ì„ ì½ì–´ì˜¤ê¸°
		frame = numpy.pad(image, ((bsi,bsi), (bsi,bsi), (0, 0)), 'constant', constant_values=(110, 110))	# ì´ë¯¸ì§€ ì£¼ë³€ì— íŒ¨ë”©ì„ ì¶”ê°€
		my  = dets['y'][fidx] + bsi  # ì–¼êµ´ bounding boxì˜ ì¤‘ì‹¬ ì¢Œí‘œ y
		mx  = dets['x'][fidx] + bsi  # ì–¼êµ´ bounding boxì˜ ì¤‘ì‹¬ ì¢Œí‘œ x
		face = frame[int(my-bs):int(my+bs*(1+2*cs)),int(mx-bs*(1+cs)):int(mx+bs*(1+cs))]	# ì–¼êµ´ í´ë¦½ì„ ì¶”ì¶œí•©
		vOut.write(cv2.resize(face, (224, 224)))	
	audioTmp    = cropFile + '.wav'				# ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì„¤ì •
	audioStart  = (track['frame'][0]) / 25		# ì˜¤ë””ì˜¤ì˜ ì‹œì‘ ì‹œê°„ì„ ê³„ì‚°
	audioEnd    = (track['frame'][-1]+1) / 25	# ì˜¤ë””ì˜¤ì˜ ì¢…ë£Œ ì‹œê°„ì„ ê³„ì‚°
	vOut.release()			# ë¹„ë””ì˜¤ ì‘ì„± ê°ì²´ë¥¼ í•´ì œ
	command = ("ffmpeg -y -i %s -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 -threads %d -ss %.3f -to %.3f %s -loglevel panic" % \
		      (args.audioFilePath, args.nDataLoaderThread, audioStart, audioEnd, audioTmp)) 
	output = subprocess.call(command, shell=True, stdout=None) 
	_, audio = wavfile.read(audioTmp)	# ì˜¤ë””ì˜¤ íŒŒì¼ LOAD
	command = ("ffmpeg -y -i %st.avi -i %s -threads %d -c:v copy -c:a copy %s.avi -loglevel panic" % \
			  (cropFile, audioTmp, args.nDataLoaderThread, cropFile)) # ì˜¤ë””ì˜¤ì™€ ë¹„ë””ì˜¤ íŒŒì¼ì„ ê²°í•©
	output = subprocess.call(command, shell=True, stdout=None)
	os.remove(cropFile + 't.avi')	# ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì‚­ì œ
	return {'track':track, 'proc_track':dets}	# ì¶”ì  ì •ë³´ì™€ ì²˜ë¦¬ëœ ì¶”ì  ì •ë³´ë¥¼ ë°˜í™˜

def extract_MFCC(file, outPath):
	# mfcc ì¶”ì¶œ
	sr, audio = wavfile.read(file)
	mfcc = python_speech_features.mfcc(audio,sr) # MFCCë¥¼ ì¶”ì¶œ
	featuresPath = os.path.join(outPath, file.split('/')[-1].replace('.wav', '.npy'))	# íŠ¹ì§• íŒŒì¼ì˜ ì €ì¥ ê²½ë¡œë¥¼ ì„¤ì •
	numpy.save(featuresPath, mfcc)	# ì¶”ì¶œëœ MFCCë¥¼ íŠ¹ì§• íŒŒì¼ë¡œ ì €ì¥


def evaluate_network(files, args):
	# sevenTeam ëª¨ë¸ì„ ì‚¬ìš©í•œ í™œì„±í™”ëœ í™”ì ê°ì§€
	s = talkNet()
	s.loadParameters(args.ASD_Model)	# ASD ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ LOAD
	sys.stderr.write("Model %s loaded from previous state! \r\n"%args.ASD_Model)
	s.eval()	# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
	allScores = []	# ëª¨ë“  ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
	# durationSet = {1,2,4,6}  # ê²°ê³¼ë¥¼ ë” ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
	durationSet = {1,1,1,2,2,2,3,3,4,5,6} 
	for file in tqdm.tqdm(files, total = len(files)):
		fileName = os.path.splitext(os.path.basename(file))[0] # ì˜¤ë””ì˜¤ì™€ ë¹„ë””ì˜¤ë¥¼ ë¡œë“œ
		_, audio = wavfile.read(os.path.join(args.pycropPath, fileName + '.wav'))
		audioFeature = python_speech_features.mfcc(audio, 16000, numcep = 13, winlen = 0.025, winstep = 0.010)
		video = cv2.VideoCapture(os.path.join(args.pycropPath, fileName + '.avi'))
		videoFeature = []
		while video.isOpened():
			ret, frames = video.read()
			if ret == True:
				face = cv2.cvtColor(frames, cv2.COLOR_BGR2GRAY)
				face = cv2.resize(face, (224,224))
				face = face[int(112-(112/2)):int(112+(112/2)), int(112-(112/2)):int(112+(112/2))]
				videoFeature.append(face)
			else:
				break
		video.release()
		videoFeature = numpy.array(videoFeature)
		length = min((audioFeature.shape[0] - audioFeature.shape[0] % 4) / 100, videoFeature.shape[0])
		audioFeature = audioFeature[:int(round(length * 100)),:]
		videoFeature = videoFeature[:int(round(length * 25)),:,:]
		allScore = [] # ëª¨ë¸ì„ ì‚¬ìš©í•œ í‰ê°€ ê²°ê³¼
		for duration in durationSet:
			batchSize = int(math.ceil(length / duration))
			scores = []
			with torch.no_grad():
				for i in range(batchSize):
					inputA = torch.FloatTensor(audioFeature[i * duration * 100:(i+1) * duration * 100,:]).unsqueeze(0).cuda()
					inputV = torch.FloatTensor(videoFeature[i * duration * 25: (i+1) * duration * 25,:,:]).unsqueeze(0).cuda()
					embedA = s.model.forward_audio_frontend(inputA)
					embedV = s.model.forward_visual_frontend(inputV)	
					out = s.model.forward_audio_visual_backend(embedA, embedV)
					score = s.lossAV.forward(out, labels = None)
					scores.extend(score)
			allScore.append(scores)
		allScore = numpy.round((numpy.mean(numpy.array(allScore), axis = 0)), 1).astype(float)
		allScores.append(allScore)	
	return allScores

def visualization(tracks, scores, args):
	# ë¹„ë””ì˜¤ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
	flist = glob.glob(os.path.join(args.pyframesPath, '*.jpg'))	
	flist.sort()
	faces = [[] for i in range(len(flist))]		# ê° í”„ë ˆì„ì— ëŒ€í•œ ì–¼êµ´ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±
	for tidx, track in enumerate(tracks):
		score = scores[tidx]
		for fidx, frame in enumerate(track['track']['frame'].tolist()):
			s = score[max(fidx - 2, 0): min(fidx + 3, len(score) - 1)] # average smoothing
			s = numpy.mean(s)
			faces[frame].append({'track':tidx, 'score':float(s),'bbox':track['track']['bbox'][fidx],'s':track['proc_track']['s'][fidx], 'x':track['proc_track']['x'][fidx], 'y':track['proc_track']['y'][fidx]})
	firstImage = cv2.imread(flist[0])
	fw = firstImage.shape[1]
	fh = firstImage.shape[0]
	vOut = cv2.VideoWriter(os.path.join(args.pyaviPath, 'video_only.avi'), cv2.VideoWriter_fourcc(*'XVID'), 25, (fw,fh))
	colorDict = {0: 0, 1: 255}
	face_score = [[] for i in range(len(flist))] 	# ê° í”„ë ˆì„ì— ëŒ€í•œ ì–¼êµ´ ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±
	for fidx, fname in tqdm.tqdm(enumerate(flist), total = len(flist)):
		image = cv2.imread(fname)
		for face in faces[fidx]:
			clr = colorDict[int((face['score'] >= 0))]
			txt = round(face['score'], 1)
			box = face['bbox']
			for tidx, track in enumerate(tracks):
				break_value = False
				for frame in track['track']['frame'].tolist():
					if frame == fidx:
						face_score[frame].append({'frame':frame, 'score':txt, 'bbox': face['bbox'], 'track': tidx})
						break_value = True
						break
				if break_value == True:
					break

			# ì„ì‹œ
			h = abs(box[1] - box[3])
			w = abs(box[2] - box[0])

			#cv2.rectangle(image, (int(box[0]-0.3*w), int(box[1]-0.3*h)), (int(box[2]+0.3*w), int(box[3]+0.3*h)),(clr,0,255-clr),10)		# ì–¼êµ´ ì£¼ë³€ì— 1.6ë°° í° íŒŒë€ ì‚¬ê°í˜•ì„ ê·¸ë¦½ë‹ˆë‹¤, í™”ìê°€ ì•„ë‹ ì‹œì—ëŠ” ë¹¨ê°„ ì‚¬ê°í˜•
			#cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),(0,clr,255-clr),10)		# ì–¼êµ´ ì£¼ë³€ì— ì‘ì€ ì´ˆë¡ ì‚¬ê°í˜•ì„ ê·¸ë¦½ë‹ˆë‹¤, í™”ìê°€ ì•„ë‹ ì‹œì—ëŠ” ë¹¨ê°„ ì‚¬ê°í˜•
			#cv2.putText(image,'%s'%(txt), (int(face['x']-face['s']), int(face['y']-face['s'])), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,clr,255-clr),5)	# ì ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‚¬ê°í˜• ìœ„ì— í‘œì‹œí•©ë‹ˆë‹¤.
		vOut.write(image)	# ë¹„ë””ì˜¤ ì‘ì„±ì„ ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ì €ì¥
	vOut.release()			# ë¹„ë””ì˜¤ ì‘ì„±ì„ ì¢…ë£Œ
	savescorePath = os.path.join(args.pyworkPath, 'scoring.pckl')
	with open(savescorePath, 'wb') as fil:      
			pickle.dump(face_score, fil)
	command = ("ffmpeg -y -i %s -i %s -threads %d -c:v copy -c:a copy %s -loglevel panic" % \
		(os.path.join(args.pyaviPath, 'video_only.avi'), os.path.join(args.pyaviPath, 'audio.wav'), \
		args.nDataLoaderThread, os.path.join(args.pyaviPath,'video_out.avi'))) 
	output = subprocess.call(command, shell=True, stdout=None)	# ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ë¥¼ ê²°í•©



# Main function
def main():
	print("=" * 50)
	print()
	print('TalkNet ëª¨ë¸ ì‹¤í–‰ ì¤‘...')
	print()

	start = time.time()

	# Initialization 
	args.pyaviPath = os.path.join(args.savePath, 'pyavi')
	args.pyframesPath = os.path.join(args.savePath, 'pyframes')
	args.pyworkPath = os.path.join(args.savePath, 'pywork')
	args.pycropPath = os.path.join(args.savePath, 'pycrop')
	if os.path.exists(args.savePath):
		rmtree(args.savePath)
	os.makedirs(args.pyaviPath, exist_ok = True) # ì…ë ¥ ë¹„ë””ì˜¤, ì…ë ¥ ì˜¤ë””ì˜¤, ì¶œë ¥ ë¹„ë””ì˜¤ë¥¼ ì €ì¥í•  ê²½ë¡œë¥¼ ìƒì„±
	os.makedirs(args.pyframesPath, exist_ok = True) # ëª¨ë“  ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì €ì¥
	os.makedirs(args.pyworkPath, exist_ok = True) # ì´ í”„ë¡œì„¸ìŠ¤ì˜ ê²°ê³¼ë¥¼ ì €ì¥
	os.makedirs(args.pycropPath, exist_ok = True) # ì´ í”„ë¡œì„¸ìŠ¤ì—ì„œ ê²€ì¶œëœ ì–¼êµ´ í´ë¦½(ì˜¤ë””ì˜¤ + ë¹„ë””ì˜¤)ì„ ì €ì¥

	# ë¹„ë””ì˜¤ ì¶”ì¶œ
	args.videoFilePath = os.path.join(args.pyaviPath, 'video.avi')
	# durationì´ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì „ì²´ ë¹„ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° 'args.start'ë¶€í„° 'args.start + args.duration'ê¹Œì§€ì˜ ë¹„ë””ì˜¤ë¥¼ ì¶”ì¶œ
	if args.duration == 0:
		command = ("ffmpeg -y -i %s -qscale:v 2 -threads %d -async 1 -r 25 %s -loglevel panic" % \
			(args.videoPath, args.nDataLoaderThread, args.videoFilePath))
	else:
		command = ("ffmpeg -y -i %s -qscale:v 2 -threads %d -ss %.3f -to %.3f -async 1 -r 25 %s -loglevel panic" % \
			(args.videoPath, args.nDataLoaderThread, args.start, args.start + args.duration, args.videoFilePath))
	subprocess.call(command, shell=True, stdout=None)
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Extract the video and save in %s \r\n" %(args.videoFilePath))
	
	# ì˜¤ë””ì˜¤ ì¶”ì¶œ
	args.audioFilePath = os.path.join(args.pyaviPath, 'audio.wav')
	command = ("ffmpeg -y -i %s -qscale:a 0 -ac 1 -vn -threads %d -ar 16000 %s -loglevel panic" % \
		(args.videoFilePath, args.nDataLoaderThread, args.audioFilePath))
	subprocess.call(command, shell=True, stdout=None)
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Extract the audio and save in %s \r\n" %(args.audioFilePath))

	# ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ
	command = ("ffmpeg -y -i %s -qscale:v 2 -threads %d -f image2 %s -loglevel panic" % \
		(args.videoFilePath, args.nDataLoaderThread, os.path.join(args.pyframesPath, '%06d.jpg'))) 
	subprocess.call(command, shell=True, stdout=None)
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Extract the frames and save in %s \r\n" %(args.pyframesPath))

	# ë¹„ë””ì˜¤ í”„ë ˆì„ì— ëŒ€í•œ ì¥ë©´ íƒì§€
	scene = scene_detect(args)
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Scene detection and save in %s \r\n" %(args.pyworkPath))	

	# ë¹„ë””ì˜¤ í”„ë ˆì„ì— ëŒ€í•œ ì–¼êµ´ íƒì§€
	faces = inference_video(args)
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Face detection and save in %s \r\n" %(args.pyworkPath))

	# Face ì¶”ì 
	allTracks, vidTracks = [], []
	for shot in scene:
		if shot[1].frame_num - shot[0].frame_num >= args.minTrack: # minTrack í”„ë ˆì„ë³´ë‹¤ ì‘ì€ shot í”„ë ˆì„ì€ ì œì™¸
			allTracks.extend(track_shot(args, faces[shot[0].frame_num:shot[1].frame_num])) # 'frames' : íŠ¸ë™ì˜ ì‹œê°„ ë‹¨ê³„, 'bbox' : ì–¼êµ´ì˜ ìœ„ì¹˜
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Face track and detected %d tracks \r\n" %len(allTracks))

	# Face í´ë¦½ ìë¥´ê¸°
	for ii, track in tqdm.tqdm(enumerate(allTracks), total = len(allTracks)):
		vidTracks.append(crop_video(args, track, os.path.join(args.pycropPath, '%05d'%ii)))
	savePath = os.path.join(args.pyworkPath, 'tracks.pckl')
	with open(savePath, 'wb') as fil:
		pickle.dump(vidTracks, fil)
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Face Crop and saved in %s tracks \r\n" %args.pycropPath)
	fil = open(savePath, 'rb')
	vidTracks = pickle.load(fil)

	# í™œì„±í™”ëœ í™”ì ê°ì§€
	files = glob.glob("%s/*.avi"%args.pycropPath)
	files.sort()
	scores = evaluate_network(files, args)
	savePath = os.path.join(args.pyworkPath, 'scores.pckl')
	with open(savePath, 'wb') as fil:
		pickle.dump(scores, fil)
	sys.stderr.write(time.strftime("%Y-%m-%d %H:%M:%S") + " Scores extracted and saved in %s \r\n" %args.pyworkPath)

	visualization(vidTracks, scores, args)

	end = time.time()

	print()
	print('TalkNet ëª¨ë¸ ì‹¤í–‰ ì™„ë£Œ')
	print(f"ğŸ•’ê±¸ë¦° ì‹œê°„ : {end - start:.2f} sec")
	print()

if __name__ == '__main__':
    main()
